{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth: Human interpretation of desired feature lines\n",
    "Date: March 20th, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and Process\n",
    "1. Meeting on Wednesday, March 8th, 2023 (Professor Ju, Professor Frachetti, Jason): \n",
    ">Professor Frachetti has some Ground Penetrating Radar (GPR) data of Tashbulak. \n",
    "><br>&emsp;GPR sends a radar signal (electormagnetic pulse). Different materials undeground reflect this signal at different speeds. We can use this difference to figure out what different materials are underground. \n",
    "><br>&emsp;Citation: https://www.nps.gov/articles/000/preservation-in-practice-gpr.htm\n",
    "><br>&emsp;GPR's data shows walls buried underground. Their pattern suggests some connected rooms\n",
    "><br>&emsp;However, everything feature underground does not necessarily show up on the ground surface, which LiDAR captures\n",
    "><br>Professor Ju suggests that we use a tool from *Yixin Zhuang, Ming Zou, Nathan Carr, Tao Ju. Anisotropic geodesics for live-wire mesh segmentation. Pacific Graphics 2014* (I refer to it as *Anistropic* below).\n",
    "><br>&emsp;A person can draw lines on the mesh that reflect a human's recognition of (the line representation of) geometric features (I refer to these hand-drawn lines as *GroundTruth_human_Anistropic*)\n",
    "\n",
    "2. Meeting on Tuesday, March 14th, 2023 (Professor Frachetti, Jason)\n",
    ">Professor Frachetti reviewed the Mesh for Tugunbulak I reconstructed from Friday, March 10th using AnalysisPackage_v1.ipynb (k values: 3 and 6). He points out that while k=3 has more noise (short/discontinous feature lines that do not capture features of interest), it captures smaller features in greater detailsâ€“some of which k=6 misses.\n",
    "><br>Professor Ju suggests that: \n",
    "><br>&emsp;I compare results from AnalysisPackage_v1.ipynb (I refer to as *CrestCODE_output* below) against *GroundTruth_human_Anistropic* to see how well *CrestCODE_output* holds up\n",
    "><br>&emsp;I obtain *GroundTruth_human_Anistropic* with smaller crops of the entire Tugunbulak mesh (File: crop_remote_230314), since *Anistropic* freezes on large meshes\n",
    "\n",
    "3. Work on Thursday, March 16th, 2023 (Jason)\n",
    ">I tried to use *Anistropic* for *GroundTruth_human_Anistropic* in the computers at Jubel Hall, WUSTL. \n",
    "><br>The entire mesh (File: crop_remote_230314.ply), which has 430,245 vertices, freezes *Anistropic*. Therefore, I need smaller crops \n",
    "><br>There are two options to crop:\n",
    "><br>&emsp;1. crop the point cloud, then reconstruct\n",
    "><br>&emsp;The problem with this method is, this takes more time (cropping point cloud and then reconstructing mesh from the cropped point cloud). Also, we get a blank 'border' with no features of interest. This is an artefact from Meshlab's screened Poisson reconstruction. \n",
    "><br>&emsp;2. crop the mesh directly\n",
    "><br>&emsp;The problem is, Meshlab does not allow me to cur through triangles. I either include or not-include a triangle in my crop. So the crop I get has messy edges.\n",
    "><br>Therefore, I decided to try using Blender, an open-source mesh-modeling tool I have lots of experience working with. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Ground Truth based on Human Visual Recognition in Blender\n",
    "Date: March 17th, 2023\n",
    "1. Difficulty with *Anistropic*\n",
    "\n",
    "1.1 Mesh size limit\n",
    ">I cropped three smaller patches from the main Tugunbulak mesh. Their number of vertices >range from 50,000 to 60,000\n",
    "><br>The problem is, they still make *Anistropic* freeze.\n",
    "><br>If I reduce the size further, I get less than 10 features of interest per crop, so I need more crops\n",
    "\n",
    "1.2 Drawing lines\n",
    ">*Anistropic* has a built-in tool for me to draw on the mesh. I can draw lines on top of the mesh and export those lines\n",
    "><br>The problem is, I am working with a mouse for this tool (it is for Windows), and I have trouble drawing accurate lines quickly\n",
    "\n",
    "2. Drawing lines in Blender\n",
    ">I found two ways to draw in Blender:\n",
    "><br>&emsp;1 Connect vertices with edges\n",
    "><br>&emsp;This method is slow. I need to create a starting vertex, and then extrude new vertices from this vertex (if I extrude vertex A from B, there will be an edge connecting A and B). This is a lot of clicks when I am trying to draw a curve!\n",
    "><br>&emsp;2 Use the Grease Pencil tool\n",
    "><br>&emsp;This method is great. I can draw freely and accurately. Since I am used to working with a trackpad for 3D modeling projects, this speed things up a lot!\n",
    "><br>For both methods, I can use the 'snap to face' feature, so that vertices on the lines I draw coincide with the first face they encounter when moved in a certain direction. (This takes 1 click)\n",
    "><br>Of course, that direction is the orthogonal Z direction!\n",
    "\n",
    "3. Results from Blender: (I refer to as *GroundTruth_human_blender*)\n",
    "\n",
    "Date: March 19th, 2023\n",
    ">The results are highlighted in Orange (Blender's default colorization for selected objects)\n",
    "\n",
    "<img src=\"../Open3D_Test/Phase_3_EntireRegion/GroundTruth_human/blender_230319_enhanced.png\" style=\"width: 800px\">\n",
    "\n",
    ">Comparison with results from Comparison_230310.ipynb (k=3)\n",
    "\n",
    "<img src=\"../Open3D_Test/Phase_3_EntireRegion/Comparison_230310/v4_path=10_k=3_lines.png\" style = \"width: 800px\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing Feature Lines in Blender\n",
    "see Drawing_Feature_Lines_Blender.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
